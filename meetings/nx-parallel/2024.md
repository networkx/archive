2025:

- **Meeting link**: https://colgate.zoom.us/j/281534728
- **Notes**: https://hackmd.io/tF0saJhyQ2i25e8tWiBrmw

# 2024

# nx-parallel meetings

meeting link: https://colgate.zoom.us/j/281534728

notes: https://hackmd.io/vlGWcfRsQF64OTIm7bCtpw?both

### Date - 25th Oct, 2024

**Attendees** : Aditi, Derek

### Topics

- https://github.com/networkx/nx-parallel/pull/86
- https://github.com/networkx/nx-parallel/pull/72
- https://scikit-learn-general.narkive.com/LR2bD7eS/grid-search-joblib#:~:text=Post%20by%20Satrajit%20Ghosh%0Adoes,scope%20of%20joblib%2C%20I%20believe


### Date - 11th Oct, 2024

**Attendees** : Aditi, Rick, Dan

### Topics

- temporary fixing https://github.com/networkx/networkx/issues/7671 : https://github.com/networkx/nx-parallel/pull/87
    - test this at networkx's level
- discussions on `spatch`
- Tests for backends - to check if the entry_points and other things are working fine for each backend
    - these can be centralised tests that backends can use

### Topics for next meeting

- handling cases like in https://github.com/networkx/nx-parallel/pull/72
    - i.e. `Parallel(n_jobs=(no_of_primary - 1) ** 2, require="sharedmem")`

### Date - 4th Oct, 2024

**Attendees** : Aditi, Derek

### Topics

- Demo of a really nice (private) networkx backend by Derek!
- discussions on distributed support in nx-parallel
    - https://ml.dask.org/joblib.html
    - https://joblib.readthedocs.io/en/latest/auto_examples/parallel/distributed_backend_simple.html
    - https://docs.dask.org/en/stable/deploying.html#distributed-computing

- Discussion on https://github.com/networkx/nx-parallel/pull/86
    - Derek will address the review comments and then get more perspectives on it
- Should we add the graph conversion `if` condition in the same decorator that we use for setting configs or a separate decorator or just leave it how it is right now i.e. not in a decorator?
    - more perspectives needed on this

### Date - 27th Sept, 2024

**Attendees** : Aditi, Derek

### Topics

- https://github.com/networkx/nx-parallel/pull/80 got merged!! :tada::tada:
- `execute_parallel` function's concept
    - move `create_iterables` inside it
        - should we have `"nodes"`, `"edges"`, `"isolates"`, `"pairs"` in `create_iterables`? 
            - [Aditi] This seems restricting in some ways.

- derek will create a PR about it!

### Date - 20th Sept, 2024

**Attendees** : Aditi, Derek

### Topics

- making PR https://github.com/networkx/nx-parallel/pull/80 ready to be merged!
- update the pre-commit hooks for them to act more uniformly across machines, editors etc.
- chunks of size `n` or `n` number of chunks?
    - memory management or time
    - we do pass `n_jobs` as `n` in the `chunks` function
    - but smaller chunks might be better but also bad in terms of memory as more the number of chunks more will be the memory usage. and what about time?
- https://joblib.readthedocs.io/en/latest/auto_examples/memory_basic_usage.html , https://github.com/scikit-learn/scikit-learn/blob/6fd2f8372227ac394cd3dd5cd96d110fa49355b8/sklearn/pipeline.py
- Adding caching in nx-parallel : https://joblib.readthedocs.io/en/latest/auto_examples/nested_parallel_memory.html#sphx-glr-auto-examples-nested-parallel-memory-py
    - shared memory not possible in nx-parallel
- [Derek] more flexibility in nx-parallel is possible beyond allowing the modification of parallel parameters from `joblib.parallel_config`

### Date - 13th Sept, 2024

**Attendees** : Aditi, Derek, Erik

### Topics

- Type annotation - removing it from PR#80 for now and make an issue on it instead
    - Are we going to test them if we have them?
    - nx_cugraphs - https://github.com/scientific-python/docstub
    - Type annotation :  sklearn doesn’t use it

- Too early to tell if we should include Map-Reduce in nx-parallel but we could have a general infrastructure to simplify things

- Heatmaps makes sense 
    - Avg to reduce noise?
    - plotting and timing in pytest-benchmarks?
    - Interactive plot (dash) - https://dash.plotly.com/interactive-graphing 
        - Algo drop-down
        - Live plot - function of vertices, edge probability is a config?
    - https://docs.rapids.ai/api/cugraph/stable/nx_cugraph/nx_cugraph/
    - https://github.com/python-graphblas/graphblas-algorithms

- Config - should we sync the two configs i.e. networkx's and joblib's?
- Speeding up generator functions - removing nested for loop might help!
- adding graph conversion and chunking logic in a decorator; include in the config decorator only?


## Notes related to nx-parallel (from the last NetworkX comm. meeting)

### Date - 4th Sept, 2024

### Topics:

- TODO : Add a parallel implementations for finding all cycles in a graph in nx-parallel([@Nikoleta-v3](https://github.com/Nikoleta-v3) requires this)
- TODO : Better testing of configs and chunking in nx-parallel
- Having a map-reduce arch. in nx-parallel? My(Aditi) main concerns 
    - might not be very first-time contributor friendly; 
    - and might not be able to fit future non-embarrassingly parallel graph algo’s implementations into this arch.
    - for reference - [PR#7](https://github.com/networkx/nx-parallel/pull/7), [Issue#30](https://github.com/networkx/nx-parallel/issues/30)
- [Derek] Adding another types of parallelism in nx-parallel —> running same algorithm on multiple graphs or subgraphs within a graph in parallel
    - Why would someone not use joblib for this kind of thing but instead would want to use nx-parallel?
        - If those graphs have a common property so the parallelism would be done in a particular way.
- Other topics:
    - benchmarking - https://github.com/networkx/nx-parallel/issues/12
    - improving configs in nx-parallel? - https://github.com/networkx/nx-parallel/issues/76
    - handling already fast algorithms - https://github.com/networkx/nx-parallel/issues/79, https://github.com/networkx/nx-parallel/issues/77
    - showcasing speedups in a better way - https://github.com/networkx/nx-parallel/issues/51 (also memory usage and other CPU usage, processes related, threads related statistics, etc.)

- TODO : Creating a website for nx-parallel (Aditi)
- related to [PR#80](https://github.com/networkx/nx-parallel/pull/80)
    - Thoughts on having type annotations in nx-parallel?
    - Why is `GraphIteratorType` a class and not just a list? and should we have all types in a separate `types.py`?
- Introducing lower-level parallelism in nx-parallel?
- 

---

---


# GSoC'24(nx-parallel) Meetings

notes: https://hackmd.io/xlZEjShuR9uUNaXBzCsrCQ?both

## Date : 22nd August, 2024

**Attendees**: Aditi, Dan

### Topics

- Create an issue on really fast function not showing any speedups in nx-parallel with links to some closed PRs(https://github.com/networkx/nx-parallel/pull/74 , https://github.com/networkx/nx-parallel/pull/44) 
- Configs : edit `Config.md`
    - duplicate context manager gets created
    - [this example](https://github.com/networkx/nx-parallel/pull/75#discussion_r1725574266) in `Config.md`

- Issue on joblib creating 2 extra functions
- Possible names for the decorator: `set_up_configure`, `_set_nx_config`, `apply_nx_config`, `allow_nx_config`, `_configure_context`, `configure_if_nx`, `configure_if_nx_active`, `setup_nx_config`
    - `_configure_if_nx_active` seems good!

- add `_set_nx_config` in nxp main namespace(like networkx do with `_dispatchable`)
    - https://stackoverflow.com/questions/2360724/what-exactly-does-import-import
    - https://docs.python.org/3/reference/simple_stmts.html#the-import-statement

- summarize https://github.com/networkx/nx-parallel/issues/51 in a comment

- share the draft GSoC's final report


## Date : 15th August, 2024

**Attendees**: Aditi, Dan

- Code from the meeting:
    ```python=
    import networkx as nx
    from joblib import parallel_config

    print(nx.config)
    nx.config.backends.parallel.n_jobs = 5

    nx.config.backends.parallel = parallel_config

    G = nx.path_graph(10)
    nx.square_clustering(G, backend="parallel")

    parallel_config(n_jobs=3)
    nx.square_clustering(G, backend="parallel")

    with parallel_config(n_jobs=7):
        nx.square_clustering(G, backend="parallel")
        with parallel_config(n_jobs=None):
            nx.square_clustering(G, backend="parallel")
            with parallel_config(n_jobs=2):
                nx.square_clustering(G, backend="parallel")

    # with nx.config(backend_priority=[parallel, cugraph], configs=[cugraph_config, nx_parallel_config]):
        # nx.square_clustering(G, backend="parallel")
    ```
- We want `joblib.parallel_config()` to be present in `nx.config.backends.parallel`.

## Date : 18th July, 2024

**Attendees**: Aditi, Dan


## Date : 4th July, 2024

**Attendees**: Aditi, Dan

### Topics

- Created conda feedstock for nx-parallel
- Setuptools PR - merged!
- Discussion on Config

## Date : 27th June, 2024

**Attendees**: Aditi, Dan

### Topics

- PyRustLin meetup:
    - audience: mostly beginners in Python
    - had interesting discussions after the 10-min talk
- created a [recipe PR](https://github.com/conda-forge/staged-recipes/pull/26768) for nx-parallel (for ref. [NetworkX's feedstock](https://github.com/conda-forge/networkx-feedstock))
    - dependent on https://github.com/networkx/nx-parallel/pull/69 (will probably merge by this end of the week)
    - [Ask Mridul] About maintainers
- GIL and python 3.13 plans and its impact on nx-parallel : https://docs.python.org/3.13/whatsnew/3.13.html
- How much inspiration to take from https://github.com/networkx/nx-parallel/pull/7 and if we can have discussions with the author of this PR.
- [Dan] scikit-learn references for nx-parallel config
    - https://github.com/scikit-learn/scikit-learn/issues/29302
    - https://github.com/scikit-learn/scikit-learn/issues/20717
- Why are they deprecating these?: https://scikit-learn.org/stable/api/deprecated.html
- Config PR in NetworkX - [Erik's reply]( https://github.com/networkx/networkx/pull/7485#issuecomment-2195042192) 
    - It was supposed to be 
    ```.py
    with nx.config(backend=backend_name, backend_configs={}):
        # nx code
    ```
    in [this](https://github.com/networkx/networkx/pull/7485#pullrequestreview-2144913888) comment.


# GSoC'24(nx-parallel) Meetings

## Date : 13th June, 2024

**Attendees**: Aditi, Dan

### Topics

- discussing [PR 7398: colliders and v_structures](https://github.com/networkx/networkx/pull/7398)
    - `stacklevel` behaves differently in terminal Python and IPython
- https://github.com/networkx/nx-parallel/pull/69

#### From developer summit:
    
- [spatch](https://github.com/scientific-python/spatch) - like nx-j4f
- scikit image backend(inspired from networkx backend dispatching) : [cuCIM](https://github.com/rapidsai/cucim)
- pandas extension(by Erik) : `pd.Dataframe.nx`

#### From last week:

- [TODO-Aditi] How sklearn deals with logging and config context manager? - ref. [sklearn context_manager](https://github.com/scikit-learn/scikit-learn/blob/8d0b243cb53ff609d32ecd7aafc5c098381eac86/sklearn/_config.py#L212), (maybe) just using joblib's default `verbose` for logging


# GSoC'24(nx-parallel) Meetings

## Date : 6th June, 2024

**Attendees**: Aditi, Dan

### Topics

- discussing [PR 7398](https://github.com/networkx/networkx/pull/7398)

#### From developer summit:
    
- [spatch](https://github.com/scientific-python/spatch) - like nx-j4f
- scikit image backend(inspired from networkx backend dispatching) : [cuCIM](https://github.com/rapidsai/cucim)
- pandas extension(by Erik) : `pd.Dataframe.nx`

#### From last week:

- [TODO-Aditi] How sklearn deals with logging and config context manager? - ref. [sklearn context_manager](https://github.com/scikit-learn/scikit-learn/blob/8d0b243cb53ff609d32ecd7aafc5c098381eac86/sklearn/_config.py#L212), (maybe) just using joblib's default `verbose` for logging
- [TODO-Dan] reviewing [PR#63](https://github.com/networkx/nx-parallel/pull/63) - Merged :tada: 

## Date : 23rd May, 2024

**Attendees**: Aditi, Dan

### Topics

- discussing proposal updates
- [TODO-Mridul] VM access for heatmaps
- different timing functions
    - `time = (initial non-parallel part of algo) + avg or max (time taken by all parallel processes) + (non-parallel ending part)` is this a good way? and can we actually do this?  
    - `timeit.default_timer` seems the best option so far(it is used by asv benchmarks)
    - reasons for not using  `time.time` (ref [blog](https://superfastpython.com/time-time-vs-timeit/#Dont_Use_timeit_for_Benchmarking_More_Complicated_Code))
- increasing the default number of times ASV runs an algorithm for benchmarking(change the default `repeat` parameter i.e. 2)
- should the `Client` for dask and ray backends be on the networkx's side(ref. [issue](https://github.com/joblib/joblib/issues/1563))
    - Mridul made a comment on this in one of the arongodb's presentations
    - dealing with registered_backends in joblib?
- probably should keep the joblib's `verbose` logging different from the networkx's logging. Two types of logging:
    - logging each parallel process, batch_size etc. (what `verbose` outputs)
    - logging which networkx backend is being used and which backend is joblib using. Does joblib use logging to see which backend is being implemented
- [TODO-Aditi] How sklearn deals with logging and config context manager?
- [TODO-Dan] reviewing [PR#63](https://github.com/networkx/nx-parallel/pull/63)
- 



---

---

[original](https://hackmd.io/MBQfVs6lSaar_ARM9jHxNw) + weekly work updates

# NetworkX Internship - 2024
Link: https://colgate.zoom.us/j/92619161786

## February 28, 2024

Attendees: Aditi, Dan, Mridul

### Topics:

- https://github.com/networkx/nx-parallel/pull/49

### Work update : Week 8 - 21st Feb to 29th Feb 2024
#### Worked on:
- improved all_pairs_bellman_ford_path algo, yielding instead of returning a generator, chunking and added get_chunks kwarg, updated heatmap 
- Bench chunking(summarised and closed) 
- MAINT: updating backend.get_info 
- renaming backend info keys in networkx’s backend.py 
- improved all all_pairs_ algos(added improved heatmaps, benchmarks and get_chunks to all algos), added 2 more algos and updated threading backend to loki for all_pairs_node_connectivity 
- Reviewed PR - addition of a unit test in bipartite module
- [DOC, DISPATCH]: updated and added `backend.py`'s docs 
- added chunking and get_chunk, and added the improved heatmap to node redundancy (in the bipartite module) and square clustering 
- added get_chunks to johnson 

## February 21, 2024

Attendees: Aditi, Dan

### Topics:

- returning a generator VS yielding
- when will we need to use a different backend through joblib?(examples - todo Aditi)

### Work update : Week 7 - 14th Feb to 20th Feb 2024
#### Worked on:
- DOC: Adding CONTRIBUTING.md, updating readme, etc.
- [DOC, DISPATCH]: updated and added `backend.py`'s docs
- Bench chunking(added benchmark results with updated algorithms to the PR) 
- added get_chunk(updated test for get_chunk so that workflow tests don’t fail) 
- MAINT: updating backend.get_info(renaming keys) 
- centrality/reaching.py(added heatmaps, but no speedups; investigated on what happens on recursively calling joblib.Parallel) 
- node_redundancy in bipartite(added heatmap) : PR
- all_pairs_ algos(added heatmaps) 
- MAINT: removed `NETWORKX_GRAPH_CONVERT`

## February 14, 2024

Attendees: Mridul, Aditi, Dan

### Topics:

- No speed ups in `global_reaching_centrality` and `local_reaching_centrality`.
- `Error: The log was not found. It may have been deleted based on retention settings.` for `test_betweenness_centrality_get_chunks` with 1100 node graph(but it might not have anything to do with the size of the graph bcoz the same error is showing up in the lint test with a smaller test graph).


### To do:
- for bipartite.node_redundancy need to use different graph generation function(or add checks?), bcoz of `nx.NetworkXError(
networkx.exception.NetworkXError: Cannot compute redundancy coefficient for a node that has fewer than two neighbors.)` https://github.com/networkx/nx-parallel/pull/45
- running chunking vs no chunking benchmarks (https://github.com/joblib/joblib/issues/1543)
- handling `edge_attr`, `node_attr`, etc. : how to handle this on the nx-parallel side
- Merge approved PRs.
- check that renaming `is_strongly_connected` is the right approach: https://github.com/networkx/nx-parallel/pull/32

### Work update : Week 6 - 7th Feb to 13th Feb 2024
#### Worked on:
- Bench chunking(figured the issue with chunking with generators) 
- added get_chunk to betweenness_centrality and additional tests for get_chunk(smoke test)
- Added 2 algos(global_reaching_centrality, local_reaching_centrality) in centrality/reaching.py
- Added node_redundancy in bipartite 
- Added heatmap with speedups for johnson algo and square clustering algos 
- updated and added benchmarks and docs in all_pairs_ algos PR
- [merged]Added default_benchmark_timeout to asv.conf.json


## February 7, 2024

Attendees: Mridul, Aditi, Dan

### Topics:

1. benchmarks for node chunk vs individual nodes
    - how does joblib's `batch_size` fit into this?
        - joblib was set to "auto"(default) for these tests.
        
2. when to use shared resources between jobs
    - what happens with very large graphs and copying it to each job?

### To do:

- [Aditi] Graph size = RAM size (https://github.com/networkx/networkx/issues/5922)
- [Aditi] multiple args in the subset func or look up the outer namespace, in a parallel setting
- [Mridul] backends docs status

### Work update : Week 5 - 31st Jan to 6th Feb 2024
#### Worked on:
- MAINT: Styling fixes 
- MAINT : added `seed` to `gnm_random_graph` in `community/tests/test_label_propagation.py`
- added self-loops related docs and tests for functions in `cluster.py`
- Implemented chunking and no chunking versions of all 7 algorithms in nx-parallel and benchmarked them on VM and fixed a few bugs
- Added heatmap, benchmark, and docs for johnson algo 
- Style fixes and rebasing in square clustering 
- Style fixes modified ParallelGraph class in all_pairs_ algos PR 
- MAINT : Added default_benchmark_timeout to asv.conf.json


## January 31, 2024

Attendees: Mridul, Aditi, Dan

### Topics:

1. how do we decide when to chunk and compute and when to iterate over nodes, compute individual nodes on each core?(https://github.com/networkx/nx-parallel/blob/main/nx_parallel/algorithms/centrality/betweenness.py#L16 VS https://github.com/networkx/nx-parallel/blob/main/nx_parallel/algorithms/shortest_paths/weighted.py#L8) 
    - can yield one node at a time(if running nodes in parallel, instead of node_chunks) , speed up depends on chunks
    - chunking might be helpful when combining the results after parallel computation.
    - startup and tear down cost of chunks VS individual nodes
    - joblib's batch_size and node chunk : https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html
    - chunking speed up in cases like `number_of_isolates` (where the individual node's computation takes negligible time)?
2. when to use shared resources?
    1. chunking and computing each chunks on different cores and then combining the result_chunks(dicts) at the end to get the final_result(combined dictionary) : https://github.com/networkx/nx-parallel/blob/3ea018c816a5ce7a89f9f99e0e1387501d988b76/nx_parallel/algorithms/shortest_paths/weighted.py#L68 (Johnson)
    2. just like 1 but computing individual nodes(in parallel) instead of node_chunks : https://github.com/networkx/nx-parallel/blob/7eb5c614dd629b5c1ef78d1f9da4defed1441bbe/nx_parallel/algorithms/cluster.py#L7 (square_clustering)
    3. chunking and having shared resources(backend=“threading”, default is “loky”), had to create an empty “all_pairs”(the shared resource) dict of dicts to begin with here, uses threads not processes : https://github.com/networkx/nx-parallel/blob/0dd112bfe06fa284520f773a11dfe299814e8b0b/nx_parallel/algorithms/approximation/connectivity.py#L10 (all_pairs_node_connectivity)

- When we use joblib.Parallel with backend="threading", it creates user-level threads. The threading backend in Joblib is based on the threading module in Python. However, due to the Global Interpreter Lock present in the CPython interpreter, which allows only one thread to execute Python bytecode at a time, threading might not be well-suited for parallelizing CPU-bound tasks in Python. but it can be useful for tasks that are I/O-bound, as threads can make progress during I/O operations, such as reading or writing to files. For CPU-bound tasks, using processes (backend="loky" in Joblib) might be more effective, as each process gets its own Python interpreter with its own GIL. (ref. https://docs.python.org/3/library/threading.html , https://joblib.readthedocs.io/en/latest/parallel.html#shared-memory-semantics , https://stackoverflow.com/questions/46657885/how-to-write-to-a-shared-variable-in-python-joblib)

- [Mridul] Python 3.13 might remove GIL

- global config is over written by hard coded `return_as="generator"`

- inspiration from libraries dependent on joblib(for chunking etc.)

- drop in replacements and configuration independent of code implementation

- [Aditi]having a config for kwargs like `get_chunk` on nx-parallel's side

### Topics for next meet:

- nxp's backend docs issue
- additional tests(for `get_chunks`)

### Work update : Week 4 - 22nd Jan to 30th Jan 2024
#### Worked on:
1. MAINT: updated test_edge_current_flow_betweenness_partition to handle symmetry cases 
2. [merged]MAINT: style fixes in nx-parallel repo 
3. ENH: added johnson to nx-parallel 
4. Updated PR to add only 7 “all_pairs_” algos to nx-parallel
5. Fixed implementation of square_clustering in nx-parallel
6. renaming tournament like in the main networkx repo
7. [merged]updated to remove rename tournament function in a separate PR and resolved conflict with the main branch(backend_info PR) 
8. Backend func_info keys renaming PR : used walrus operator to make the code concise 
9. Added notes for self-loops in transitivity and generalized_degree and added test_self_loops_square 
10. [merged]dfs sort_neighbors : made sort_neighbors keyword only arg 
11. [merged]updated pre-commit-config 
12. [merged]added linting wf test 
#### Issues:
1. Unexpected test failures in test_algebraic_connectivity.py(in nx-parallel) (ref. Issue)
2. Unexpected test failures in test_edge_current_flow_betweenness_partition in nx-parallel for windows (ref. Issue)
3. [Discussion]self-loops consideration for clustering functions (ref. Discussion)

## January 24, 2024

Attendees: Aditi

Only one attendee.

### Work update : Week 3 - 15th Jan to 21st Jan 2024
#### Worked on:
1. [Ready for review] added square_clustering algo’s implementation to nx-parallel (ref. [PR](https://github.com/networkx/nx-parallel/pull/34))
2. [Ready for review] adding backend_info entry pt to nx-parallel : after discussing, decided to remove redundant docs and updated the get_info accordingly, made this PR independent from PR 7219(renaming PR) and created separate PR for pre-commit-config file (ref. [PR](https://github.com/networkx/nx-parallel/pull/27))
3. [Ready for review] updating pre-commit-config.yaml (ref. [PR](https://github.com/networkx/nx-parallel/pull/35))
4. [Ready for review] renaming backend `func_info` dictionary's keys : decided to remove “backend_func_examples” and made the code more concise by using the walrus operator (ref. [PR](https://github.com/networkx/networkx/pull/7219))
5. [Quick merge] added `time_tournament_is_strongly_connected` benchmark (ref. [PR](https://github.com/networkx/nx-parallel/pull/32))
6. [Merged] added lint wf test (ref. [PR](https://github.com/networkx/nx-parallel/pull/28))
7. [Ready for review] Adding sort_neighbors to DFS funcs : made the definition of get_children concise and resolved conflicts with the main branch (ref. [PR](https://github.com/networkx/networkx/pull/7196))
8. [WIP] Adding get_chunks kwarg : decided to remove joblib related kwargs and use a config dict instead, and added get_chunk kwarg (ref. [PR](https://github.com/networkx/nx-parallel/pull/29))
9. [Ready for review] added `all_pairs` functions to nx-parallel (ref. [PR](https://github.com/networkx/nx-parallel/pull/33))
10. [Not ready for review] experimented with adding parallelism to circleci in networkx - WIP (ref. [PR](https://github.com/networkx/networkx/pull/7231))
11. [Merge at EoC] removed TODO comment in tournament func (ref. [PR](https://github.com/networkx/networkx/pull/7226))

#### Issues closed/opened:
1. [resolved]No attributes 'from_numpy_matrix' and 'to_numpy_matrix' (ref. [Issue](https://github.com/networkx/networkx/issues/7239)) 
2. [opened]`test_ring_of_cliques` test fails unexpectedly (ref. [Issue](https://github.com/networkx/networkx/issues/7240))



## January 17, 2024

Attendees: Mridul, Aditi, Dan

### Topics:

Docs: Lets add a /doc directory to nx-parallel with 2 main documents:
    - a webpage that describes the option in joblib that we can control through backend_config with examples of how to change them.
    - a tutorial 

Both Dan and Aditi should send ssh key to Mridul to get us set up with a benchmarking cloud compute machine he is setting up. This link describes how to do that -- only send the public key -- and should be similar on Mac and only slightly different on windows.
https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-22-04

### Work update : Week 2 - 8th Jan to 14th Jan 2024

#### Worked on:
1. Renaming and adding backend func_info dict’s keys: 2 renamed(“extra_docstring” to “backend_func_docs” and “extra_parameters” to “additional_parameters”) and 2 keys added(“backend_func_url” and “backend_func_examples” (ref. [PR](https://github.com/networkx/networkx/pull/7219))
2. adding backend_info entry point (the get_info func) and updating docs for all funcs accordingly (ref. [PR](https://github.com/networkx/nx-parallel/pull/27))
    - maybe split get_info part into a separate PR
3. Added 4 more joblib-related kwargs and create_iterables functions to utils (ref. [PR](https://github.com/networkx/nx-parallel/pull/29))
4. [Approved]adding sort_neighbors to dfs funcs : changing name from sort_children to sort_neighbor, removing “callable” and returning the error to the user instead, and updating the docs accordingly (ref. [PR](https://github.com/networkx/networkx/pull/7196))
5. removing “TODO : This can be trivially parallelized” comments from networkx codebase (ref. [PR](https://github.com/networkx/networkx/pull/7226))
#### PRs reviewed :
1. Add nx.backend_config dict (ref. [PR](https://github.com/networkx/networkx/pull/7225 ))
#### Issues :
1. Refactoring and Simplification Suggestions(based on PR #7) : reviewed PR#7 in nx-parallel repo and created a detailed summary having suggestions and general analysis and the reasoning behind all of them (ref. [Issue](https://github.com/networkx/nx-parallel/issues/30))
    
## January 10, 2024

Attendees: Mridul, Aditi, Dan

### Topics:

- documentation:
    - adding to networkx main docs. Currently NX loads a dict from the backend that contains the text for each function.  See an example here: [graphblas's dict](https://github.com/python-graphblas/graphblas-algorithms/blob/main/_nx_graphblas/__init__.py) and [backend_info](https://github.com/python-graphblas/graphblas-algorithms/blob/35dbc90e808c6bf51b63d51d8a63f59238c02975/pyproject.toml#L70)(automate making the dictionary)
    - should we have a documentation website for nx-parallel? We might want one at some later point, but for now let's hold off until we have a better idea of what nx-parallel will be providing.
- networkx PRs
     - nice work here... reviewing as well as contributing. :)
- nx-parallel new algorithms
    - find TODO with "trivially parallelizable"
    - find old PR/Issues that talk about parallel and implement tose functions
- which joblib parameters should we have set via the function and which can be set outside the function signature? 
        - context manager(advantage : if joblib adds more parameters) : adding tests for this
        - function args(disadvantage : overloads the function calls)
    - we like the joblib context manager approach. Let's include in nx-parallel some tests that use the joblib context manager
- should we include functions that let users choose their own chunking?
    - let's wait to provide this. What situations might someone want to do this?
        - if the user has info on which nodes might take a lot of time -- they can split them so they are evenly distributed in each chunk.
        - chunking with some random seed

### Work update : Week 1 - 1st Jan to 7th Jan 2024

#### Worked on:
1. updated pre-commit-config.yaml and docs for all funcs(added nx-parallel specific egs and docs), tried adding n_jobs kwarg, researched how joblib and sklearn configure it (ref. [PR](https://github.com/networkx/nx-parallel/pull/27))
2. added lint work-flow test and made style fixes accordingly (ref. [PR](https://github.com/networkx/nx-parallel/pull/28))
3. Created a separate PR for adding joblib-related kwargs(later decided to have a config. dictionary instead) (ref. [PR](https://github.com/networkx/nx-parallel/pull/29))
4. adding sort_children to dfs funcs : had discussions on iterable VS iterator and modifying the code and docs accordingly in dfs and bfs functions (ref. [PR](https://github.com/networkx/networkx/pull/7196))
5. [merged]Updated docstring in panther_similarity (ref. [PR](https://github.com/networkx/networkx/pull/7175))
6. [merged]Replaced “plugin” with “backend” and updated README in nx-parallel (ref. [PR](https://github.com/networkx/nx-parallel/pull/26))
7. [merged]Developed a benchmarking infrastructure : changed config files and updated README (ref. [PR](https://github.com/networkx/nx-parallel/pull/24))
#### PRs co-reviewed :
1. corner cases in rich_club_coefficient (ref. [PR](https://github.com/networkx/networkx/pull/7212))
2. corner cases in random_spanning_tree (ref. [PR](https://github.com/networkx/networkx/pull/7211))

## January 3, 2024 - Kickoff 2024 meeting

Attendees: Aditi, Dan

### Topics:

- doc_strings (nx vs nx-parallel)
    - we are assuming that most of the users of nx-parallel are coming from networkx, so the docs should be written accordingly
    - we discussed including description of how we parallelize the algorithm. Include parameters not in the networkx function and maybe all the parameters for completeness??. Benchmarking should be part of the docs, but maybe separate from the function descriptions. We'll try this and see if it needs changing.
    - The 2-5 lines of documentation we put into the NetworkX documentation are important too because they invite the user to the nx-parallel package.
- n_jobs default value
    - let's stick with the joblib name n_jobs and use joblib's default value and handling of negative values.
- testing
    - run network tests with nx-parallel backend. Look for types of tests that would be helpful to test the parallel aspects. Look for algos which give different results based on number of cpus. 
- benchmarking
    - parameter : type of graph generation method(barabasi_albert_graph, real world graphs, fast_gnp_random_graph, small world graphs)
    - using different OSs
    - benchmarks on SNAP datasets
- extra
    - we decided we should have functions in nx-parallel that are already in networkx(flexible)

## December 20, 2023 - Kickoff meeting

Attendees: Aditi, Mridul, Dan

### Topics:
- Documentation?
    - Should nx-parallel have it's dedicated doc website?
    - Each function which is implemented by nx-parallel should have a link in it's NX docs. The backend docs need to have information about extra keyword args or args that have constraints.

        - for example, there should be an argument for the number of jobs that can be run at once.

- Benchmarks?
    - Where to keep an encompassing benchmark which uses all the backends? To bring up in community/dispatching.
    - Adding memory benchmarks. asv has some memory measurement systems. Also [memray](https://github.com/bloomberg/memray) was mentioned.
    - benchmarks for how good(accurate) the approximation is, for approx. algos?

- Testing?
    - One idea is to just use networkx tests. [Graphblas-algorithms has this](https://github.com/python-graphblas/graphblas-algorithms/blob/main/graphblas_algorithms/tests/test_match_nx.py)
    - Can run locally for nx-parallel:
    ```NETWORKX_GRAPH_CONVERT=parallel \
          NETWORKX_TEST_BACKEND=parallel \
          NETWORKX_FALLBACK_TO_NX=True \
                python -m pytest --pyargs networkx

          python -m pytest --doctest-modules --pyargs nx_parallel```
